\documentclass{article}
\usepackage[margin=2.5cm, includefoot, footskip=30pt]{geometry}

\setlength{\parindent}{0em}
\setlength{\parskip}{1em}
\renewcommand{\baselinestretch}{1}

%%%%Packages%%%%
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{graphics}
\usepackage{multicol}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{color,colortbl}
\usepackage{array}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{wrapfig, blindtext}
\usepackage{soul}
\usepackage[table]{xcolor}
%%%%%%%%%%%%%%%%%

\definecolor{Gray}{gray}{0.92}
\usepackage[first=0,last=9]{lcg}
\newcommand{\ra}{\rand0.\arabic{rand}}

\newcommand{\uniquenumberofseeds}{\input{unique_number_of_seeds.tex}}
\newcommand{\numberofalltournaments}{\input{number_of_all_tournaments.tex}}
\newcommand{\numberofstrategies}{\input{unique_number_of_strategies.tex}}

\setlength{\tabcolsep}{3pt}

\title{Properties of winning Iterated Prisoner's Dilemma strategies.}
\author{Nikoleta E. Glynatsi, Vincent A. Knight, Marc Harper}
\date{}

\begin{document}

\maketitle

\begin{abstract}
Researchers have explored the performance of Iterated Prisoner's Dilemma strategies
for decades: from the celebrated performance of Tit for Tat, to the
introduction of the zero-determinant strategies, to the use of sophisticated learning
structures such as neural networks, many new strategies have been introduced and tested
in a variety of tournaments and population dynamics. Typical results in the literature,
however, rely on performance against a small number of somewhat arbitrarily selected
strategies in a very small number of tournaments, casting doubt on the generalizability
of conclusions. We analyze a large collection of \numberofstrategies typically known
strategies in \numberofalltournaments tournaments, present the top performing strategies across multiple
tournament types, and distill their salient features. The results show that there is not yet a single
strategy that performs well in diverse Iterated Prisoner's Dilemma scenarios.
Nevertheless there are several properties that heavily influence the best performing
strategies, refining the properties described by R. Axelrod in light of
recent and more diverse opponent populations. These are: be nice, be provocable and contrite,
be a little envious, be clever, and adapt to the environment, which includes the parameters
of the tournament (e.g. noise) and the population of opponents. More precisely,
we find that strategies perform best when their probability of cooperation
matches the total tournament population's aggregate cooperation probabilities,
or a proportion thereof in the case of noisy and probabilistically ending tournaments,
and that the manner in which a strategy achieves the ideal cooperation rate is crucial.
The features of high performing strategies reveal why strategies such as Tit For Tat
performed historically well in tournaments and why zero-determinant strategies
typically do not fare well in tournament settings.
\end{abstract}

\section{Background}

The Iterated Prisoner's Dilemma (IPD) is a repeated two player game that models
behavioural interactions, specifically interactions where
self-interest clashes with collective interest. At each turn of the game both
players, simultaneously and independently, decide between cooperation (C) and
defection (D), given memory of their prior interactions. The payoffs for each
player, at each turn, is influenced by their own choice and the choice of the
other player. The payoffs of the game are defined by:

\[\begin{pmatrix}
R & S \\
T & P
\end{pmatrix}\]

where typically \(T > R > P > S\) and \(2R > T + S\). The most common values used in
the literature~\cite{Axelrod1981} are $R=3, P=1, T=5, S=0$. These values are also
used in this work.

Conceptualising strategies and understanding the best way of playing the game
has been of interest to the scientific community since the formulation of the
game in 1950~\cite{Flood1958}. Following the computer tournaments of Axelrod in the
1980's~\cite{Axelrod1980a, Axelrod1980b}, a strategy's performance in a round
robin computer tournament became a common evaluation technique for newly designed
strategies. Many tournaments have followed Axelrod's~\cite{Bendor1991, Beaufils1997, Stephens2002,
Kendall2007, Stewart2012, Harper2017} and today more hundreds of strategies exist
in the literature.

The
winner of both of Axelrod's tournaments~\cite{Axelrod1980a,
Axelrod1980b} was the simple strategy Tit For Tat (TFT) which cooperated
on the first turn and then simply copied the previous action of its opponent,
retailiating against defections with a defection, and forgiving a defection if followed
by a cooperation. Axelrod concluded that the strategy's robustness was due to four
properties, which he adapted into four suggestions on doing well in an IPD:

\begin{itemize}
    \item Do not be envious by striving for a payoff larger than the opponent's payoff
    \item Be "nice"; Do not be the first to defect
    \item Reciprocate both cooperation and defection; Be provocable to retaliation and forgiveness
    \item Do not be too clever by scheming to exploit the opponent
\end{itemize}

As a result of the strategy's strong performance in both tournaments, and moreover in a
series of evolutionary experiments~\cite{Axelrod1981}, TFT was often
claimed to be the most robust basic strategy in the IPD.

There are strategies which have built upon TFT and the reciprocity based approach.
In~\cite{Beaufils1997} Gradual was introduced which was constructed to have the
same qualities as those of TFT except one: Gradual had a memory
of the game since the beginning of it. Gradual recorded the number of defections
by the opponent and punished them with a growing number of defections. It would
then enter a calming state in which it would cooperates for two rounds. A
strategy with the same intuition as Gradual is Adaptive Tit for
Tat~\cite{tzafestas-2000a}. Adaptive Tit for Tat maintains a continually updated estimate of the
opponent's behaviour, and uses this estimate to condition its future actions.

Other works have built upon the limitations of TFT, and others have shown
that suggestions made by Axelrod did not necessarily apply in alternative environmental settings.
In~\cite{Bendor1991, Donninger1986, Molander1985, Hammerstein1984} it was shown
that TFT suffered in environments with noise. This was
mainly due to the strategy's lack of generosity and contrition. Since TFT immediately
punishes a defection, in a noisy environment it can get stuck in a
repeated cycle of defections and cooperations. Some new strategies, more
robust in tournaments with noise, were soon introduced, including
Nice and Forgiving~\cite{Bendor1991}, Generous Tit For Tat~\cite{Nowak1992},
and Pavlov (aka Win Stay Lose Shift)~\cite{Nowak1993}, as well as later
variants such as OmegaTFT \cite{kendall2007iterated}. Introduction of new strategies
is often accompanied by a claim that the new strategy is the best known despite
only being tested against a small number of opponents, or specific classes of
opponents not necessarily representative of all possible or all published strategies.
The lack of testing against formally defined strategies and tournament winners
is understandable given the effort required to implement
\footnote{Implementing prior strategies faithfully is often extremely difficult or impossible
due to insufficient descriptions and lack of published implementations or code.}
the hundreds of published IPD strategies, yet calls into question any claims of superiority
or robustness of newly introduced
strategies.

A set of envious IPD strategies were introduced called zero-determinant strategies (ZDs)
in~\cite{Press2012}. By
forcing a linear relationship between stationary payoffs ZDs can ensure that they will
never receive less than their memory-one opponents. While ZDs
were introduced with a small tournament in which some were reportedly
successful \cite{Stewart2012}, this result has not generally held in future
work. In~\cite{Harper2017} a
series of strategies trained using reinforcement learning were introduced, and a tournament containing over
200 strategies featured no ZD strategies ranked in top
spots. Instead, the top ranked strategies were a set of ``clever''
(in the sense of Axelrod's characteristics) trained
strategies based on lookup tables~\cite{Axelrod1987}, hidden Markov
models~\cite{Harper2017}, and finite state automata~\cite{Miller1996}.
Similarly, in \cite{mathieu2017}, a set of evolutionarily-trained strategies,
and a pre-selected set of known strategies, outperformed a collection of 12 ZDs.

Though only select pieces of work have been discussed, there is a broad collection
of strategies in the literature, and new strategies and competitions are being
published frequently~\cite{Glynatsi2019}. The question, however, still remains
the same: what is the best way to play the game?

Compared to other works, where typically a few selected or introduced strategies
are evaluated on a small number of tournaments and/or small number of opponents,
this manuscript evaluates the performance of \numberofstrategies
strategies in \numberofalltournaments tournaments. Furthermore a large portion
of the strategies used in this manuscript are drawn from the known and named strategies
in IPD literature, including many previous tournament winners,
in contrast to other work that may have randomly generated many essentially arbitrary
strategies (typically restrained to a class such as memory-one strategies,
or those of a certain structural form such as finite state machines or deterministic
memory two strategies). Additionally, our tournaments come in a
number of variations including standard tournaments emulating Axelrod's original tournaments,
tournaments with noise, probabilistic match length, and both noise and probabilistic match length.
This diversity of strategies and tournament types yields new insights and tests
earlier claims in alternative settings against known powerful strategies.

The later part of the paper evaluates
the impact of features on the performance of the strategies using modern
machine learning techniques. These features include measures regarding a
strategy's behaviour and measures regarding the tournaments. The outcomes of our
work reinforce the discussion started by Axelrod, and it concludes that the properties
of a successful strategy in the IPD are:

\begin{itemize}
    \item \st{Do not be envious} Be a little bit envious
    \item Be "nice"; Do not be the first to defect
    \item Reciprocate both cooperation and defection; Be provocable and forgiving
    \item \st{Do not be too clever} It's ok to be clever
    \item Adapt to the environment; Adjust to the mean population cooperation
\end{itemize}

The different tournament types as well as the data collection, made
possible due to an open source library called Axelrod-Python (APL),
are covered in Section~\ref{section:data_collection}. The data set generated
for this work has been made publicly available~\cite{data} and can be used
for further analysis and insights.
Section~\ref{section:top_performances}, focuses on the best performing
strategies for each type of tournament and overall.
Section~\ref{section:evaluation_of_performance}, explores the traits which
contribute to good performance, and finally the results are summarised in
Section~\ref{section:conclusion}. This manuscripts uses several parameters,
introduced in the following sections. The full set of
parameters and their definitions are given in Appendix~\ref{app:parameters}.

\section{Data collection}\label{section:data_collection}

The data set generated for this manuscript was created with APL version 3.0.0.
APL allows for different types of IPD computer
tournaments to be simulated and contains a large list of strategies.
Most of these are strategies described in the literature with a few exceptions
of strategies that have been contributed specifically to the package. This
paper makes use of \numberofstrategies strategies implemented in version 3.0.0. A
list of the strategies is given in the Appendix~\ref{app:list_of_players}.
Although APL features several tournament types, this work considers
standard, noisy, probabilistic ending, and noisy probabilistic ending
tournaments.

\textbf{Standard tournaments} are tournaments similar to that of Axelrod's well-known
tournaments ~\cite{Axelrod1980a}. There are \(N\) strategies which all play an iterated
game of \(n\) number of turns against each other. Note that self-interactions
are not included. Similarly, \textbf{noisy
tournaments} have \(N\) strategies and \(n\) number of turns, but at each turn
there is a probability \(p_n\) that a player's action will be flipped.
\textbf{Probabilistic ending tournaments}, are of size \(N\) and after each turn
a match between strategies ends with a given probability \(p_e\). Finally,
\textbf{noisy probabilistic ending} tournaments have both a noise probability
\(p_n\) and an ending probability \(p_e\). For smoothing the simulated results a
tournament is repeated for \(k\) number of times. This was allowed to vary 
in order to evaluate the effect of smoothing. The winner of each tournament
is based on the average score a strategy achieved and not by the number of wins.

The process of collecting tournament results is described by
Algorithm~\ref{algorithm:data_generation}. For each trial a random size \(N\) is
selected, and from the \numberofstrategies strategies a random list of \(N\) strategies is
chosen. For the given list of strategies a standard, a noisy, a probabilistic
ending and a noisy probabilistic ending tournament are performed and repeated
\(k\) times. The parameters for the tournaments, as well as the number of
repetitions, are selected once for each trial. The parameters and their
respective minimum and maximum values are given by
Table~\ref{table:parameters_values}.

\begin{table}[!htbp]
    \begin{center}
        \resizebox{.6\textwidth}{!}{
        \begin{tabular}{lcccc}
    \toprule
    parameter & parameter explanation &   min value & max value \\
    \midrule
    $N$ & number of strategies  & 3 & 195 \\
    $k$ & number of repetitions  & 10 & 100 \\
    $n$ & number of turns      & 1 & 200 \\
    $p_n$ & probability of flipping action at each turn  & 0 & 1   \\
    $p_e$ & probability of match ending in the next turn & 0 & 1   \\
    \bottomrule
        \end{tabular}}
    \end{center}
    \caption{Data collection; parameters' values}
    \label{table:parameters_values}
\end{table}

The source code for the data collection, as well as the source code for
the analysis, which will be discussed in the following sections, have been written
following best practices~\cite{Aberdour2007, Benureau2018}
and is available here. %TODO archive code

\begin{algorithm}[!htbp]
    \setstretch{1.35}
    \ForEach{\text{seed} $\in [0, 11420]$}{
        $N \gets \text{randomly select integer}\in [N_{min}, N_{max}]$\;
        $\text{players} \gets  \text{randomly select $N$ players}$\;
        $k \gets  \text{randomly select integer}\in [k_{min}, k_{max}]$\;
        $n \gets  \text{randomly select integer}\in [n_{min}, n_{max}]$\;
        $p_n \gets  \text{randomly select float}\in [p_{n\, min}, p_{n\, max}]$\;
        $p_e \gets   \text{randomly select float}\in [p_{e\, min}, p_{e\, max}]$\;
        \vspace{0.4cm}
        $\text{result standard}$ $\gets$ Axelrod.tournament$(\text{players}, n, k)$\;
        $\text{result noisy}$ $\gets$ Axelrod.tournament$(\text{players}, n, p_n, k)$\;
        $\text{result probabilistic ending}$ $\gets$ Axelrod.tournament$(\text{players}, p_e, k)$\;
        $\text{result noisy probabilistic ending}$ $\gets$ Axelrod.tournament$(\text{players}, p_n, p_e, k)$\;

    }
    \KwRet{result standard, result noisy, result probabilistic ending,
    result noisy probabilistic ending}\;
    \caption{Data collection Algorithm}
    \label{algorithm:data_generation}
\end{algorithm}

A total of \uniquenumberofseeds trials of Algorithm~\ref{algorithm:data_generation} have been
run. For each trial the results for 4 different tournaments were collected,
thus a total of \numberofalltournaments $(\uniquenumberofseeds \times 4)$ tournament results have been
retrieved. Each tournament outputs a result summary in the form of
Table~\ref{table:output_result}. Each strategy have participated on average in
5154 tournaments of each type. The strategy with the maximum participation in each
tournament type is Inverse Punisher with 5639 entries. The strategy with the
minimum entries is EvolvedLookerUp 1 1 1 which was selected in 4693 trials.

A result summary (Table~\ref{table:output_result}) has \(N\) number of rows
because each row contains information for each strategy that participated in the
tournament. The information includes the strategy's rank, median score, the rate
with which the strategy cooperated $(C_r)$, its match win count, and the
probability that the strategy cooperated in the opening move. Moreover, the
probabilities of a strategy being in any of the four states ($CC, CD, DC, DD$),
and the rate of which the strategy cooperated after each state. The \textbf{normalised rank}
feature that is manually added. The rank \(R\) of a given
strategy can vary between 0 and \(N-1\). Thus, the normalised rank,
denoted as $r$, is calculated as a strategy's rank divided by \(N - 1\).

\newcolumntype{g}{>{\columncolor{Gray}}c}
\begin{table}[!htbp]
    \begin{center}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{ccccccgcgcgcgcg}
    \toprule 
    & & & & & &   \multicolumn{8}{g}{Rates}  \\
    Rank & Name & Median score & Cooperation rating $(C_r)$ & Win & Initial C &
    CC & CD & DC & DD & CC to C & CD to C & DC to C & DD to C \\
    0 &  EvolvedLookerUp2 2 2 & 2.97 & 0.705 & 28.0 & 1.0 & 0.639 & 0.066 & 0.189 &
    0.106 & 0.836 & 0.481 & 0.568 & 0.8 \\
    1 &  Evolved FSM 16 Noise 05 & 2.875 & 0.697 & 21.0 & 1.0 & 0.676 & 
    0.020 & 0.135 & 0.168 & 0.985 & 0.571 & 0.392 & 0.07 \\
    2 & PSO Gambler 1 1 1 & 2.874 & 0.684 &  23.0 &     1.0 &    0.651 &    0.034 &    0.152 &    0.164
    & 1.000 & 0.283 & 0.000 & 0.136 \\
    3 &  PSO Gambler Mem1 &  2.861 &        0.706 &  23.0 &      1.0 &    0.663
    &    0.042 &    0.145 &    0.150 &  1.000 &  0.510 &  0.000 &  0.122 \\
    4 &          Winner12 &  2.835 &        0.682 &  20.0 &      1.0 &
    0.651 &    0.031 &    0.141 &    0.177 &  1.000 &  0.441 &  0.000 &  0.462 \\
    $\dots$ & $\dots$ & $\dots$ & $\dots$ & $\dots$ & $\dots$ & $\dots$ & $\dots$ &
    $\dots$ & $\dots$ & $\dots$ & $\dots$ & $\dots$ & $\dots$ \\
    \bottomrule
    \end{tabular}}
\end{center}
\caption{Output result of a single tournament.}\label{table:output_result}
\end{table}

\section{Top ranked strategies}\label{section:top_performances}

The performance of each strategy is evaluated in four tournament types, as
presented in Section \ref{section:data_collection}, followed by an
evaluation of their performance over all the \numberofalltournaments simulated
tournaments of this work. Each strategy participated in multiple tournaments of
the same type (on average 5154). For example TFT participated in a
total of 5114 tournaments of each type. The strategy's normalised rank
distribution in these is given in Figure~\ref{fig:tit_for_tat_r_distribution}. A
value of \(r = 0\) corresponds to a strategy winning the tournament where a
value of \(r = 1\) corresponds to the strategy coming last. Because of the
strategies' multiple entries their performance is evaluated based on the
\textbf{median normalised rank} denoted as \(\bar{r}\).

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.60\textwidth]{../images/tit_for_tat_r_distributions.pdf}
    \caption{TFT's $r$ distribution in tournaments. Lower values of \(r\)
    correspond to better performances. The best performance
    of the strategy has been in standard tournaments where it achieved a $\bar{r}$
    of 0.34.}
    \label{fig:tit_for_tat_r_distribution}
\end{figure}

The top 15 strategies for each tournament type based on \(\bar{r}\) are given in
Table~\ref{table:top_performances}. The data collection process was designed such
that the probabilities of noise and ending of the match varied between 0 and
1. However, commonly used values of these probabilities are
values are often less than 0.1. Thus,
Table~\ref{table:top_performances} also includes the top 15 strategies in noisy
tournaments with \(p_n < 0.1\) and probabilistic ending tournaments with \(p_e <
0.1\).

\newcolumntype{g}{>{\columncolor{Gray}}l}
\begin{table}[!htbp]
    \begin{center}
    \resizebox{\textwidth}{!}{
        \input{top_perfomances.tex}
    }
\end{center}
\caption{Top performances for each tournament type based on $\bar{r}$. The
results of each type are based on 11420 unique tournaments of each type. The
results for noisy tournaments with \(p_n < 0.1\) are based on 1151 tournaments,
and for probabilistic ending tournaments with \(p_e < 0.1\) on 1139. The top
ranks indicate that trained strategies perform well in a variety of
environments, but so do simple deterministic strategies. The normalised medians
are close to 0 for most environments, except environments with noise not
restricted to 0.1 regardless the number of turns. Noisy and noisy probabilistic
ending tournaments have the highest medians. This implies that strategies from
the collection of this work do not perform well in environments with high values
of noise.}
\label{table:top_performances}
\end{table}

The \(r\) distributions for the top ranked strategies of Table~\ref{table:top_performances}
are given by Figure~\ref{fig:r_distributions}.

\begin{figure*}[!htbp]
    \centering
    \begin{subfigure}{0.485\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../images/r_distribution_standard.pdf}
        \caption{$r$ distributions of top 15 strategies in standard tournaments.}\label{fig:std_results}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.485\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../images/r_distribution_noise_subset.pdf}
        \caption{$r$ distributions of top 15 strategies in noisy tournaments with \(p_n < 0.1\).}\label{fig:noise_subset_results}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}{0.485\textwidth}
        \centering 
        \includegraphics[width=\textwidth]{../images/r_distribution_noise.pdf}
        \caption{$r$ distributions of top 15 strategies in noisy tournaments.}\label{fig:noise_results}
    \end{subfigure}
    \quad
    \begin{subfigure}{0.485\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../images/r_distribution_probend_subset.pdf}
        \caption{\(r\) distributions of top 15 strategies in 1139 probabilistic ending
        tournaments with \(p_e < 0.1\).}
        \label{fig:probend_subset_results}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}{0.485\textwidth}
        \centering 
        \includegraphics[width=\textwidth]{../images/r_distribution_probend.pdf}
        \caption{$r$ distributions of top 15 strategies in probabilistic ending tournaments.}\label{fig:probend_results}
    \end{subfigure}
    \quad
    \begin{subfigure}{0.485\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../images/r_distribution_probend_noise.pdf}
        \caption{$r$ distributions of top 15 strategies in noisy probabilistic ending tournaments.}
        \label{fig:probend_noise_results}
    \end{subfigure}
    \caption{\(r\) distributions of the top 15 strategies in different
    environments. A lower value of \(\bar{r}\) corresponds to a more successful
    performance. A strategy's \(r\) distribution skewed towards zero indicates
    that the strategy ranked highly in most tournaments it participated in. Most
    distributions are skewed towards zero except the distributions with
    unrestricted noise, supporting the conclusions from
    Table~\ref{table:top_performances}.}\label{fig:r_distributions}
\end{figure*}

In standard tournaments 10 out of the 15 top strategies are introduced
in~\cite{Harper2017}. These are strategies based on finite state automata (FSM),
hidden markov models (HMM), artificial neural networks (ANN), lookup tables
(LookerUp) and stochastic lookup tables (Gambler) that have been trained using
reinforcement learning algorithms (evolutionary and particle swarm algorithms).
They have been trained to perform well against the strategies
in APL in a standard tournament, thus their performance in the
specific setting was anticipated. DoubleCrosser, BackStabber and Fool Me Once, are
strategies not from the literature but from the APL. DoubleCrosser is an extension
of BackStabber and both strategies make use of the number of turns because they are
set to defect on the last two rounds. It should be noted that these
strategies can be characterised as ``cheaters'' because the source code of the strategies
allows them to know the number of turns in a match (unless the match has a probabilistic ending).
These strategies were expected to not perform as well in
tournaments where the number of turns is not specified. Finally, Winner
12~\cite{mathieu2017} and DBS~\cite{Au2006} are both from the literature.
DBS is a strategy specifically designed for noisy environments, however, it ranks
highly in standard tournaments as well. Similarly the fourth ranked player,
Evolved FSM 16 Noise 05, was
trained for noisy tournaments yet performs well in standard tournaments.
Figure~\ref{fig:std_results} shows that these strategies typically perform
well in any standard tournament in which they participate.

In the case of noisy tournaments with \(p_n < 0.1\) the top performed strategies
include strategies specifically designed for noisy tournaments. These are DBS,
Evolved FSM 16 Noise 05, Evolved ANN 5 Noise 05, PSO Gambler 2 2 2 Noise 05 and
Omega Tit For Tat~\cite{kendall2007iterated}. Omega TFT, another strategy designed
to break the deadlocking cycles of CD and DC that TFT can fall into in noisy
environments, places 10th. The rest of the top ranks are
occupied by strategies which performed well in standard tournaments and
deterministic strategies such as Spiteful Tit For Tat~\cite{prison}, Level
Punisher~\cite{Eckhart2015}, Eugine Nier~\cite{lesswrong}. Similarly to standard
tournaments, the successful strategies in this given setting performed well
overall in the tournaments they participated in,
Figure~\ref{fig:noise_subset_results}.

In comparison, the performance of the top ranked strategies in noisy environments
when \(p_n\in [0, 1]\) is bimodal. The top strategies include strategies which
decide their actions based on the cooperation to defection ratio, such as
ShortMem~\cite{Andre2013}, Grumpy~\cite{axelrodproject} and
e~\cite{axelrodproject}, and the Retaliate strategies which are designed to
defect if the opponent has tricked them more often than a given percentage of the times that
they have done the same. The bimodality of the \(r\) distributions is explained
by Figure~\ref{fig:effect_of_noise} which demonstrates that the top 6 strategies
were highly ranked due to the their performance in tournaments with \(p_n>0.5\),
and that in tournaments with a noise probability lower than \(0.5\) they
performed poorly. At a noisy level of \(0.5\) or greater, mostly cooperative strategies
become mostly defectors and vice versa.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.92\textwidth]{../images/noise_effect.pdf}
    \caption{\(r\) distributions for top 6 strategies in noisy tournaments over
    the probability of noisy ($p_n$).}
    \label{fig:effect_of_noise}
\end{figure}

The new entrants to the most effective strategies list in probabilistic ending
tournaments with \(p_e< 0.1\) are a series of Meta strategies, trained strategies
which performed well
in standard tournaments, and Grudger~\cite{axelrodproject} and Spiteful Tit for
Tat~\cite{prison}. The Meta strategies~\cite{axelrodproject} create a team of
strategies and play as an ensemble or some other combination of their team
members. Figure~\ref{fig:probend_subset_results} indicates that these strategies
performed well in any probabilistic ending tournament they competed in.

In probabilistic ending tournaments with \(p_e \in [0, 1]\) the top ranks are
mostly occupied by defecting strategies such as Better and Better, Gradual
Killer, Hard Prober (all from~\cite{axelrodproject}), Bully (Reverse Tit For
Tat)~\cite{Nachbar1992} and Defector, and a series of strategies based on finite
state automata introduced by Daniel Ashlock and Wendy Ashlock; Fortress 3,
Fortress 4 (both introduced in~\cite{Ashlock2006}), Raider~\cite{Ashlock2014}
and Solution B1~\cite{Ashlock2014}. The success of defecting strategies in
probabilistic ending tournaments is due to larger values of
\(p_e\) which lead to shorter matches (the expected number of rounds is \(1 / p_e\)), so the
impact of the PD being iterated is subdued. This is captured by the Folk
Theorem~\cite{Fudenberg2009} as defecting strategies do better when the likelihood
of the game ending in the next turn increases.
This is demonstrated by Figure~\ref{fig:effect_of_probend}, which gives the
distributions of \(r\) for the top 6 strategies in probabilistic ending tournaments
over \(p_e\).

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.92\textwidth]{../images/folk_theorem.pdf}
    \caption{\(r\) distributions for top 6 strategies in probabilistic ending tournaments
    over $p_e$. The 6 strategies start of with a high median rank,
    however, their ranked decreased as the the probability of the game ending
    increased and at the point of \(p_e = 0.1\).}
    \label{fig:effect_of_probend}
\end{figure}

The top performances in tournaments with both noise and a probabilistic ending
and the top performances over the entire data set have the largest median values
compared to the top rank strategies of the other tournament types,
Figure~\ref{fig:probend_noise_results} and Figure~\ref{fig:overall_results}. The
\(\bar{r}\) for the top strategy is approximately at 0.3, indicating that the
most successful strategy can on average just place at the top 30\% of the
competition.

\begin{table}[!htbp]
    \centering
    \resizebox{.30\textwidth}{!}{
    \input{performance_merged_table.tex}}
    \caption{Top performances over all the tournaments. The top ranks include
    strategies that have been previously mentioned. The set of Retaliate
    strategies occupy the top spots followed by BackStabber and DoubleCrosser.
    The distributions of the Retaliate strategies have no statistical
    difference. PSO Gambler and Evolved HMM 5 are trained strategies introduced
    in~\cite{Harper2017} and Nice Meta Winner and NMWE Memory One are strategies
    based on teams. Grudger is a strategy from Axelrod's original tournament and
    Forgetful Fool Me Once is based on the same approach as
    Grudger.}\label{table:overall_results}
\end{table}

\begin{figure}[!htbp]
        \centering
        \includegraphics[width=.65\textwidth]{../images/performance_merged.pdf}
        \caption{\(r\) distributions for best performed strategies in the data set~\cite{data}.
        A lower value of \(\bar{r}\) corresponds to a more successful
        performance.}
        \label{fig:overall_results}
\end{figure}

On the whole, the analysis of this manuscript has shown that:

\begin{itemize}
    \item In standard tournaments the dominating strategies were 
    strategies that had been trained using reinforcement learning techniques.
    \item In noisy environments where the noise probability strictly less than
    0.1 was considered, the successful strategies were strategies specifically
    designed for noisy environments.
    \item In probabilistic ending tournaments most of the highly ranked
    strategies were defecting strategies and trained finite state automata, all
    by the authors of~\cite{Ashlock2006, Ashlock2014}. These strategies ranked
    high due to their performance in tournaments where the probability of the
    game ending after each turn was bigger than 0.1.
    \item In probabilistic tournaments with \(p_e\) less than 0.1 the highly
    ranked strategies were strategies based on the behaviour of others.
    \item From the collection of strategies considered here,  no strategy can be
    consistently successful in noisy environments, except if the value of noise
    is constrained to less than a 0.1.
\end{itemize}

Though there is not a single strategy that repeatably outranks all others in any
of the distinct tournament types, or even across the tournaments type, there
are specific types of strategies have been repeatably ranked in the top ranks.
These have been strategies that have been trained, strategies that retailiate,
and strategies that would adapt their behaviour based on preassigned rules
to achieve the highest outcome. These results contradict some of Axelrod's suggestions,
and more specifically, the suggestions `Do not be clever' and `Do not be envious'.
The features and properties contributing a strategy's success are further
explored in Section~\ref{section:evaluation_of_performance}.

\section{Evaluation of performance}\label{section:evaluation_of_performance}

Now we examine performance of the strategies based on features of strategies described in
Table~\ref{table:manual_features}. These features are measures regarding a
strategy's behaviour from the tournaments the strategies competed in as well as
intrinsic properties such as whether a strategy is deterministic or stochastic.

\newcolumntype{g}{>{\columncolor{Gray}}c}
\begin{table}[!htbp]
    \begin{center}
    \resizebox{.99\textwidth}{!}{
    \begin{tabular}{gcgcgc}
    \toprule
    feature & feature explanation &  source & value type & min value & max value \\
    \midrule
stochastic  &  If a strategy is stochastic & strategy classifier from APL & boolean  & Na &  Na \\
makes use of game &  If a strategy makes used of the game information & strategy classifier from APL & boolean  & Na &  Na \\
makes use of length &  If a strategy makes used of the number of turns & strategy classifier from APL & boolean  & Na &  Na \\
memory usage &  The memory size of a strategy divided by the number of turns & memory size from APL & float & 0 &  1 \\
SSE & A measure of how far a strategy is from ZD behaviour & method described in~\cite{Knight2019} & float & 0 & 1 \\
max cooperating rate $(C_{\text{max}})$  & The biggest cooperating rate in a given tournament  & result summary  & float & 0 & 1\\
min cooperating rate $(C_{\text{min}})$ & The smallest cooperating rate in a given tournament  & result summary  & float & 0 & 1\\
median cooperating rate $(C_{\text{median}})$ & The median cooperating rate in a given tournament  & result summary  & float & 0 & 1\\
mean cooperating rate $(C_{\text{mean}})$ & The mean cooperating rate in a given tournament  & result summary  & float & 0 & 1 \\
$C_r$ / $C_{\text{max}}$ & A strategy's cooperating rate divided by the maximum & result summary  & float & 0 & 1 \\
$C_{\text{min}}$ / $C_r$ & A strategy's cooperating rate divided by the minimum & result summary  & float & 0 & 1 \\
$C_r$ / $C_{\text{median}}$ & A strategy's cooperating rate divided by the median  & result summary  & float & 0 & 1\\
$C_r$ / $C_{\text{mean}}$ & A strategy's cooperating rate divided by the mean & result summary  & float & 0 & 1 \\
$C_r$ & The cooperating ratio of a strategy & result summary  & float & 0 & 1 \\
$CC$ to $C$ rate & The probability a strategy will cooperate after a mutual cooperation & result summary  & float & 0 & 1\\
$CD$ to $C$ rate & The probability a strategy will cooperate after being betrayed by the opponent & result summary  & float & 0 & 1 \\
$DC$ to $C$ rate & The probability a strategy will cooperate after betraying the opponent & result summary  & float & 0 & 1 \\
$DD$ to $C$ rate & The probability a strategy will cooperate after a mutual defection & result summary  & float & 0 & 1 \\
$p_n$ & The probability of a player's action being flip at each interaction & trial summary & float & 0 & 1 \\
$n$ & The number of turns & trial summary & integer & 1 & 200 \\
$p_e$ & The probability of a match ending in the next turn & trial summary & float & 0 & 1 \\
$N$ & The number of strategies in the tournament & trial summary & integer & 3 & 195 \\
$k$ & The number of repetitions of a given tournament & trial summary & integer & 10 & 100 \\
    \bottomrule
        \end{tabular}}
    \end{center}
    \caption{The features which are included in the performance evaluation
    analysis. Stochastic, makes use of length and makes use of game are APL
    classifiers that determine whether a strategy is stochastic or deterministic,
    whether it makes use of the number of turns or the game's payoffs. The
    memory usage is calculated as the number of turns the strategy considers to
    make an action (which is specified in the APL) divided by the number of
    turns. The SSE (introduced in~\cite{Knight2019}) shows how close a strategy
    is to behaving as a ZDs, and subsequently, in an extortionate way. The
    method identifies the ZDs closest to a given strategy and calculates the
    algebraic distance between them, defined as SSE. A SSE value of 1 indicates
    no extortionate behaviour at all whereas a value of 0 indicates that a
    strategy is behaving a ZDs. The rest of the features considered are the $CC$
    to $C$, $CD$ to $C$, $DC$ to $C$, and $DD$ to $C$ rates as well as
    cooperating ratio of a strategy, the minimum (\(C_{min}\)), maximum
    (\(C_{max}\)), mean (\(C_{mean}\)) and median (\(C_{median}\)) cooperating
    ratios of each tournament.}
    \label{table:manual_features}
\end{table}

The memory usage of strategies with an infinite memory size, for example Evolved
FSM 16 Noise 05, is equal to 1. Otherwise the memory usage is the number of
rounds of play used by the strategy divided by the number of turns in each match.
For example, Winner12 uses the previous two rounds of play, and if participating in a
participated in a tournament where $n$ was 100 the memory usage would be 2/100.
Note that for tournaments with a probabilistic
ending the number of turns was not collected, so the memory usage feature is not
used for probabilistic ending tournaments.

The correlation coefficients between the features of
Table~\ref{table:manual_features} the median score and the median normalised
rank are given by Table~\ref{table:correlations}. The correlation coefficients
between all features of Table~\ref{table:manual_features} have been calculated
and a graphical representation can be found in the
Appendix~\ref{app:correlations}.

\newcolumntype{g}{>{\columncolor{Gray}}c}
\begin{table}[!htbp]
    \begin{center}
    \resizebox{.9\textwidth}{!}{
        \input{correlations_table.tex}
    }
\end{center}
\caption{Correlations table between the features of Table~\ref{table:manual_features}
the normalised rank and the median score.}\label{table:correlations}
\end{table}

In standard tournaments the features  $CC$ to $C$, $C_r$, $C_r / C_{\text{max}}$
and the cooperating ratio compared to $C_{\text{median}}$ and $C_{\text{mean}}$
have a moderate negative effect on the normalised rank, and a moderate positive
on the median score. The SSE error and the $DD$ to $C$ have the opposite
effects. Thus, in standard tournaments behaving cooperatively corresponds to a
more successful performance. Even though being nice pays off,
that's not true against defective strategies. Being more cooperative after a mutual
defection is associated to lesser overall success in terms of normalised rank.
Figure~\ref{fig:rates_of_winners_in_standard_tournaments} confirms that the
winners of standard tournaments always cooperate after a mutual cooperation and
almost always defect after a mutual defection.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.57\textwidth]{../images/rates_of_winners_in_standard_tournaments.pdf}
    \caption{Distributions of $CC$ to $C$ and $DD$ to $C$ for the winners in
    standard tournaments.}\label{fig:rates_of_winners_in_standard_tournaments}
\end{figure}

Compared to standard tournaments, in both noisy and in probabilistic ending
tournaments the higher the rates of cooperation the lower a strategy's success
and median score. A strategy would want to cooperate less than both
the mean and median cooperator in such settings. In probabilistic ending
tournaments the correlation coefficients have larger values, indicating a
stronger effect. Thus a strategy will be punished more by its cooperative
behaviour in probabilistic ending environments, supporting the results of
Section~\ref{section:evaluation_of_performance}
as well. The distributions of the $C_r$ of the winners in
both tournaments are given by Figure~\ref{fig:c_r_distributions}. It confirms
that the winners in noisy tournaments cooperated less than 35\% of the time
and in probabilistic ending tournaments less than 10\%.
In noisy probabilistic ending tournaments and over all the tournaments' results,
the only features that had a moderate effect are $C_r/C_{\text{mean}},
C_r/C_{\text{max}}$ and $C_r$. In such environments cooperative behaviour
appears to be punished less than in noisy and probabilistic ending
tournaments.

Moreover, the manner in which a strategy achieves a given cooperation rate
relative to the tournament population average is important. Playing a strategy
that randomly cooperates with $C_{\text{mean}}$ is unlikely to be as effective as it will
randomly have cooperations matched with defections. In contrast, TFT naturally
achieves a cooperation rate near $C_{\text{mean}}$ by virtue of copying its opponent's
last move while also minimizing instances where it is exploited by an opponent
(cooperating while the opponent defects), at least in non-noisy tournaments.
\footnote{This also explains why Tit For N Tats does not fare well -- it fails
to achieve the proper cooperation ratio.}
TFT forces the opponent to pay back the (C, D) round with a (D, C) round before
returning to mutual cooperation. This explains why TFT performed well
in Axelrod's original tournaments as most strategies submitted to those tournaments
were typically cooperative and relatively few strategies used an easily exploitable
pattern. TFT does not appear in the top ranks in
these tournaments because it is too nice. Strategies like Grudger will always
defect after a fixed number of opponent defections, which allows them to
effectively exploit strategies like Alternator or stochastic strategies that
have a non-zero chance of cooperating after mutual defection, which TFT will
not do. Moreover in a noisy environment these strategies will naturally tend
toward always defecting, leading them to exploit strategies like Cooperator. In
such a scenario the noisy environment effectively voids Axelrod's rule to be nice,
allowing strategies to attempt exploitation, whereas in a noise-free environment,
exploitation is risky because several strategies exhibit a Grudger-like behavior,
reducing the overall value in attempting to exploit strategies like Cooperator.

Similarly, these results suggest an explanation regarding the intuitively unexpected
effectiveness of memory one strategies historically. Given that among the important
features associated with success are the relative cooperation rate to the population
average and the four memory-one probabilities of cooperating conditional on the
previous round of play, all five features can be optimized by a memory one
strategy such as TFT. Usage of more history becomes valuable when there are
exploitable opponent patterns, indicated by the importance of SSE as a feature,
that the first-approximation provided by a memory one strategy is no longer
sufficient.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.57\textwidth]{../images/c_r_winners_tournaments.pdf}
    \caption{$C_r$ distributions of the winners in noisy and in probabilistic
    ending tournaments.}\label{fig:c_r_distributions}
\end{figure}

A multivariate linear regression has been fitted to model the relationship between
the features and the normalised rank. Based on the graphical representation of
the correlation matrices given in Appendix~\ref{app:correlations} several of the
features are highly correlated. Highly correlated features have been removed
before fitting the linear regression model. The features included are given
by Table~\ref{table:linear_regression} alongside their corresponding \(p\) values
in the distinct tournaments and their regression coefficients.

\newcolumntype{g}{>{\columncolor{Gray}}c}
\begin{table}[h]
    \begin{center}
\resizebox{\textwidth}{!}{
    \input{regression_table.tex}}
    \end{center}
    \caption{Results of multivariate linear regressions with \(r\) as the dependent variable.
    \(R\) squared is reported for each model.}
    \label{table:linear_regression}
\end{table}

A multivariate linear regression has also be fitted on the median score. The
coefficients and \(p\) values of the features can be found in
Appendix~\ref{app:regression_median_score}. The results of the two methods
are in agreement.

The feature \(C_{r} / C_{\text{mean}}\) has a statistically significant effect
across all models and a high regression coefficient. It has both a positive and
negative impact on the normalised rank depending on the environment. For
standard tournaments, Figure~\ref{fig:discussion_standard} gives the
distributions of several features for the winners of standard tournaments. The
\(C_{r} / C_{\text{mean}}\) distribution of the winner is also given in
Figure~\ref{fig:discussion_standard}. A value of \(C_r / C_{\text{mean}} = 1\)
implies that the cooperating ratio of the winner was the same as the mean
cooperating ratio of the tournament, and in standard tournaments, the median is
1. Therefore, an effective strategy in standard tournaments was the mean
cooperator of its respective tournament.

The distributions of SSE and \(CC\) to \(D\) rate for the winners of standard
tournaments are also given in Figure~\ref{fig:discussion_standard}. The SSE
distributions for the winners indicate that the strategy behaved in a ZD way in
several tournaments, however, not constantly. The winners participated in
matches where they did not try to extortionate their opponents. Furthermore, the
\(CC\) to \(D\) distribution indicates that if a strategy were to defect against
the winners they would reciprocate on average with a probability of 0.5.

\begin{figure}[!htbp]
    \centering
        \centering
        \includegraphics[width=\textwidth]{../images/standard_discussion.pdf}
        \caption{Distributions of \(C_r / C_{\text{mean}}\), SSE and \(CD\) to \(C\) ratio
        for the winners of standard tournaments. A
        value of \(C_r / C_{\text{mean}} = 1\) imply that the cooperating ratio of the
        winner was the same as the mean cooperating ratio of the tournament. An SSE distribution
        skewed towards 0 indicates a extortionate behaviour by the strategy.}
        \label{fig:discussion_standard}
\end{figure}

Similarly for the rest of the different tournaments types, and the entire data
set the distributions of \(C_r / C_{\text{mean}}\), SSE and \(CD\) to \(C\) ratio
are given by Figures~\ref{fig:discussion_noisy},~\ref{fig:discussion_probend},
\ref{fig:discussion_probend_noisy} and~\ref{fig:discussion_entire_data}.

Based on the \(C_r / C_{\text{mean}}\) distributions the successful strategies
have adapted differently to the mean cooperator depending on the tournament
type. In noisy tournaments where the median of the distribution is at 0.67, and
thereupon the winners cooperated 67\% of the time the mean cooperator did. In
tournaments with noise and a probabilistic ending the winners cooperated 60\%,
whereas in settings that the type of the tournament can vary between all the
types the winners cooperated 67\% of the time the mean cooperator did. Lastly,
in probabilistic ending tournaments above more defecting
strategies prevail (Section~\ref{section:top_performances}), and this result is
reflected here.

\begin{figure}[!htbp]
    \centering
        \centering
        \includegraphics[width=\textwidth]{../images/noisy_discussion.pdf}
        \caption{Distributions of \(C_r / C_{\text{mean}}\), SSE and \(CD\) to \(C\) ratio
        for the winners of noisy tournaments.}
        \label{fig:discussion_noisy}
\end{figure}

The probability of noise has been observed to excessively affect optimal
behaviour. In environments with considerable values of noise no strategy from
our collection managed to perform sufficiently.
Figure~\ref{fig:compared_to_mean_over_noise_probability} gives the ratio \(C_r /
C_{\text{mean}}\) for the winners in tournaments with noise, over the
probability of noise. From Figure~\ref{fig:noisy_discussion_over_noise} it is
clear that the cooperating only 67\% of the time the mean cooperator did is
optimal only when \(p_n \in [0.2, 0.4)\) and \(p_n \in [0.6, 0.7]\). In
environments with \(p_n < 0.1\) the winners want to be close to the mean
cooperator, similarly to standard tournaments, and as the probability of noise
is exceeding 0.5 (the game becomes unreasonable) strategies should aim to be
less and less cooperative.

Figure~\ref{fig:compared_to_mean_over_noise_probability} gives \(C_r /
C_{\text{mean}}\) for the winners over \(p_n\) in tournaments with noise and a
probabilistic ending. The optimal proportions of cooperations are different
now that the number of turns is not fixed, successful strategies
want to be more defecting that the mean cooperator, that only changes when
\(p_n\) approaches 0.5. Figure~\ref{fig:compared_to_mean_over_noise_probability}
demonstrates how the adjustments to \(C_r /C_{\text{mean}}\) change over the
noise in the to the environment, and thus supports how important adapting to
the environment is for a strategy to be successful.
 
\begin{figure}[!htbp]
    \centering
    \begin{subfigure}{0.485\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../images/noisy_discussion_over_noise.pdf}
        \caption{\(C_r / C_{\text{mean}}\) distribution for winners in noisy tournaments over
        \(p_n\).}\label{fig:noisy_discussion_over_noise}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.485\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../images/noisy_probend_discussion_over_noise.pdf}
        \caption{\(C_r / C_{\text{mean}}\) distribution for winners in noisy probabilistic ending tournaments over
        \(p_n\).}\label{fig:noisy_probend_discussion_over_noise}
    \end{subfigure}
    \caption{\(C_r / C_{\text{mean}}\) distributions over intervals of \(p_n\).
    These distributions model the optimal proportion of cooperation
    compared to \(C_{\text{mean}}\) as a function of (\(p_n\)).}
    \label{fig:compared_to_mean_over_noise_probability}
\end{figure}

The distributions of the SSE across the tournament types suggest that successful
strategies exhibit some extortionate behaviour, but not constantly.
ZDs are a set of strategies that are envious as they try to exploit their
opponents. The winners of the tournaments considered in this work are
envious, but not as much as many ZDs. This highlights why TFT's early tournament
success fails to generalize -- it never attempts to defect against a cooperating
or exploitable opponent (e.g. Alternator). Moreover, many of the strategies in
the library will not tolerate exploitation attempts. A clever strategy can
achieve mutual cooperation with stronger strategies while also being able to
exploit weaker strategies. This is why ZDs fail to appear in the top
ranks -- they try to exploit all opponents and cannot actively adapt back to
mutual cooperation against stronger strategies, which requires more depth of memory.
\footnote{Note that ZDs also tend to perform poorly in population games for
a similar reason: they attempt to exploit other players using ZDs, failing to
form a cooperative subpopulation. This makes them good invaders but poor resisters of invasion.}

\begin{figure}[!htbp]
    \centering
        \centering
        \includegraphics[width=\textwidth]{../images/probend_discussion.pdf}
        \caption{Distributions of \(C_r / C_{\text{mean}}\), SSE and \(CD\) to \(C\) ratio
        for the winners of probabilistic ending tournaments.}
        \label{fig:discussion_probend}
\end{figure}

The distributions of the \(CD\) to \(C\) rate evaluate the behaviour of a
successful strategy after its opponent has defected against it. In standard
tournaments it was observed that a successful strategy reciprocates with a
probability of 0.5. This is distinct between the tournament types. In
tournaments with noise a strategy is less likely to cooperate following a
defection compared to standard tournaments, and in probabilistic ending
tournaments a strategy will reciprocate a defection. In a setting that the type
of the tournament can vary between all the examined types a winning strategy
would reciprocate on average with a probability of 0.58.

\begin{figure}[!htbp]
    \centering
        \centering
        \includegraphics[width=\textwidth]{../images/probend_noisy_discussion.pdf}
        \caption{Distributions of \(C_r / C_{\text{mean}}\), SSE and \(CD\) to \(C\) ratio
        for the winners of noisy probabilistic ending tournaments.}
        \label{fig:discussion_probend_noisy}
\end{figure}

Further statistically significant features with strong effects include \(C_r /
C_{\text{min}}\), \(C_r / C_{\text{max}}\), \(C_{\text{min}}\) and
\(C_{\text{max}}\). These add more emphasis on how important it is for a  a
strategy to adapt to its environment. Finally, the features number of turns,
repetitions and the probabilities of noise and the game ending had no
significant effects based on the multivariate regression models.

\begin{figure}[!htbp]
    \centering
        \centering
        \includegraphics[width=\textwidth]{../images/entire_data_discussion.pdf}
        \caption{Distributions of \(C_r / C_{\text{mean}}\), SSE and \(CD\) to \(C\) ratio
        for the winners over the tournaments of the entire data set.}
        \label{fig:discussion_entire_data}
\end{figure}

A third method that evaluates the importance of the features in
Table~\ref{table:manual_features} using clustering and random forests can be found in
the Appendix~\ref{app:clustering}. The results uphold the outcomes of the
correlation and multivariate regression. It also evaluates the effects
of the classifiers stochastic, make use of game, and make use of length which
have not been evaluated by the methods above because there are binary variables.
The results imply that they have no significant effect on a strategy's
performance.

\section{Discussion}\label{section:conclusion}

This manuscript has explored the performance of \numberofstrategies strategies
of the Iterated Prisoner's Dilemma in a large number of computer tournaments.
We analyzed and extracted the salient features of the best performing strategies
across various tournament types, casting the results in terms of Axelrod's
original suggested features of good IPD strategies. Moreover, our results
shed light on the historic performance of TFT, zero-determinant strategies,
and memory one strategies generally. Strategies need to match their play
to the cooperativeness of the tournament population and do so in a way that
prevents or minimizes exploitation. Overall we see that complex or clever
strategies can be effective, whether trained against a corpus of possible
opponents or purposely designed to mitigate the impact of noise such as
in the strategy DBS. Further, we showed that while the type of exploitation
attempted by ZDs is not typically effective in tournaments, more
sophisticated strategies capable of selectively exploiting weaker opponents
while mutually cooperating with stronger opponents can be highly successful.
This fact was also indicated numerically by the importance of the strategy
feature SSE in the analysis of strategy features. These results highlight
a central idea in evolutionary game theory in this context: the fitness
landscape is a function of the population (where fitness in this case is
tournament performance). While that may seem obvious now, it shows why
historical tournament results on small or arbitrary populations of strategies
have so often failed to produce generalizable results.

Highly noisy or tournaments with short matches favor less cooperative strategies.
These environments mitigate the value of being nice. Uncertainty enables
exploitation, reducing the ability of maintaining or enforcing mutual cooperation,
while triggering grudging strategies to switch from typically cooperating to
typically defecting. Accordingly, we find that in noisy tournaments
the best performing players cooperate a lower rate than the tournament
population on average. Nevertheless we found some strategies designed or trained
for noisy environments were also highly ranked in noise-free tournaments. This
indicates that strategy complexity is not necessarily a liability, rather it
can confer adaptability to a more diverse set of environments.

In Section~\ref{section:top_performances}, the tournaments results were used to
present the top performances. The data set contained results from four different
settings, and these were also studied individually. In standard tournaments
complex strategies trained using reinforcement learning ranked in the top spots.
Some of these strategies ranked again in the top spots in probabilistic
ending tournaments when a \(p_e\) of less 0.1 was considered and in noisy tournaments
when \(p_n\) was less than 0.1. In probabilistic
ending tournaments \(p_e\) was designed to vary between 0 and 1. It was demonstrated
that for values larger than 0.1, as stated in the Folk Theorem, defecting strategies
were winning the tournaments because there was a high likelihood of the game
ending in the next turn. In tournaments with noise the median ranks of the top
15 strategies had the highest values and the \(r\) distributions were bimodal.
The top rank strategies were performing both well and bad, and this indicates
that in noisy tournaments where the noise can vary substantially,
there were no strategies that can guarantee winning across a range of noise.
However, if the probability of noise was constrained at 0.1 then strategies
designed for noisy tournaments indeed performed well.

So what is the best way of playing the IPD? And is there a single dominant
strategy for the IPD? There was not a single strategy within the collection of the
\numberofstrategies strategies, that has managed to perform well in all the
tournaments variations it competed in.
Even if on average a strategy ranked highly in a specific environment it did not
guarantee its success over the different tournament types. However, the results
of sections~\ref{section:top_performances}
and~\ref{section:evaluation_of_performance} have demonstrated that there are
properties associated with the success of strategies. A few of the properties that
have been identified by this manuscript's analysis contradict the properties of
Axelrod~\cite{Axelrod1981}. Namely, in Section~\ref{section:top_performances} it
was shown that trained strategies and strategies that decided their actions
based on pre-designed strategies to maximise their utility dominated several
tournaments across tournament types, hinting that successful IPD strategies are
often clever or more complex than simple strategies like TFT. Most of the
successful strategies highlighted
in~Section~\ref{section:top_performances} were strategies that begin with
cooperation.

Furthermore, in Section~\ref{section:top_performances}
and~\ref{section:evaluation_of_performance} it was shown that envious strategies
performed well. Though these were not the most envious strategies in the
tournaments (ZDs were included), these strategies benefited by being a bit
envious. From Section~\ref{section:evaluation_of_performance} it was concluded
that there is a significant importance in adapting to the environment, and more
specifically in this work, to the mean cooperator. This section also
demonstrated that a strategy should reciprocate, as suggested by Axelrod, but in
some environments, such as standard and noisy, it should relax its readiness to
do so.

Thus, the five properties successful strategies need to have in a IPD competition
are: be nice, be provocable and contrite, be a little envious, be clever, and adapt to the
environment (including the population of strategies).

The data set described in this work contains the largest number of IPD tournaments,
to the authors knowledge, and it available at~\cite{data}. Further data mining
could be applied and provide new insights in the field.

\bibliographystyle{plain}
\bibliography{bibliography}

\section{Acknowledgements}

A variety of software have been used in this work:

\begin{itemize}
    \item The Axelrod-Python library for IPD simulations~\cite{axelrodproject}.
    \item The Matplotlib library for visualisation~\cite{hunter2007matplotlib}.
    \item The Numpy library for data manipulation~\cite{walt2011numpy}.
    \item The scikit-learn library for data analysis~\cite{scikit-learn}.
\end{itemize}

\appendix

\input{parameters_section}
\input{correlation_section}
\input{random_forest_evaluation}
\input{regression_on_median_score}
\input{strategies_list_section}

\end{document}
